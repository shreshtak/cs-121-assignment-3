DELIVERABLE/SUBMISSION
- project writeup pg 7: Come up with a set of at least 20 queries that guide you in evaluating
how well your search engine performs, both in terms of ranking performance
(effectiveness) and in terms of runtime performance (efficiency). At least half of
those queries should be chosen because they do poorly on one or both criteria;
the other half should do well. 
    - put it in TEST.txt file (no need to report results)
    - with comments on which ones started by doing poorly and explanations of what you did to make them perform better.
- project writeup pg 7: README.txt file describing how to use
your software (i.e. at lesat: how to run the code that creates the index,
how to start the search interface and how to perform a simple query) 
- don't include index files in zip
- in person demo




INDEXER
- [DONE] project writeup pg 2: Important text: text in bold (b, strong), in headings (h1, h2, h3), and
in titles should be treated as more important than the in other places. Verify which are the relevant
HTML tags to select the important words.
- [DONE] project writeup pg 4: Your indexer must off load the inverted index hash map from main memory to a 
partial index on disk at least 3 times during index construction; those partial indexes should be merged in the end.
    - do we have to merge the entirety of the indices??? yes
- [DONE] lec 18 slide 27: split inverted index into alphabet ranges
- lec 20+21+22: tf-idf scoring with cosine similarity (lec 21 slide 33, slide 56)
- [DONE] when to filter out non-alnum tokens: currently done in merge_indexes(), but what if we filtered them out in
create_partial_indexes() before adding them to partial index?




SEARCHER
- [DONE] lec 17 slide 32: query optimization: process merged posting lists for each term in ascending order of length
- [DONE] lec 18 slide 5: timer to time query times (must be under 300ms)
- lec 18 slide 27: look into what this means: The search component needs to keep files open all the time. Note: the files 
are open, but not in memory!
- rank the results based on tf-dif and cosine similarity




EXTRA CREDIT
- Detect and eliminate duplicate pages. (1 pt for exact, 2 pts for near)
- Add HITS and Page Rank to ranking. (1.5 pts HITS, 2.5 for PR)
- Implement 2-gram and 3-gram indexing and use it in retrieval. (1 pt)
- Enhance the index with word positions and use them for retrieval. (2 pts)
- Index anchor words for the target pages. (1 pt).
- Implement a Web or local GUI interface instead of using the console. (1 pt for the local GUI, 2 pts for a web GUI)
    - flask (easier according to ed) or streamlit
- Implement the summarization of the resulting pages using the OpenAI API (or a local model using LangChain)
and show the short summaries in your web GUI (note: you need a web GUI for this bonus point!). (2 pts)



IDEA 2
# open all the inverted index files and store them in a dictionary with letter as the key
inverted_index_files = {}
for letter in a-z and numbers 0-9:  # ignore non-alnum tokens
    inverted_index_files[letter or number] = open(/../inverted_index_{letter or number}, 'w')

# open all the partial index files and store them in an array
partial_index_files = []
num_partial_index_files = 0
for partial_index in partial_index_paths:
    partial_index_files.append(open(partial_index, 'r'))
    num_partial_index_files++

partial_index_lines = {}    # to store the ((token, i), posting list) pairs to be merged
merged_lists = {}           # to store the (token, merged posting list) pairs to be written to disk
merged_list_count = 0       # to keep track of how many tokens in merged_lists

# execute loop while there are still partial index files to read
while num_partial_index_files > 0:
    for i, partial_index_file in enumerate(partial_index_files):
        read line and separate into token and posting list
        if not empty string:
            put (token, i) into priority queue
            eval the postings list and put it in partial_index_lines with (token, i) as key
        else:
            # EOF reached
            pop partial_index_file from partial_index_files
            num_partial_index_files--

    get first (token, i) from priority queue
    continue popping (token, i) pairs from priority queue while tokens are the same
    for all the docs that contain the token, pop the entries from partial_index_lines and merge the posting lists together
    store merged list in merged_lists with token as key
    merged_list_count++
    
    if merged_list_count == SOME NUMBER:
        write the merged lists to the appropriate alphabetic inverted_index_file
        clear merged_lists and reset merged_list_count

if merged_lists_count > 0:
    write whatever is left in merged lists to inverted_index_files


good queries:
    cristina lopes (69 ms)
    machine learning (132 ms)
    acm (46 ms)

bad queries:
    a computer (368 ms)
    master of software engineering (520 ms)